{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Website not found\n",
      "\n",
      "No emails found\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from googlesearch import search\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\"\n",
    "}\n",
    "\n",
    "EMAIL_REGEX = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
    "\n",
    "def find_website(company_name):\n",
    "    query = f\"{company_name} official website\"\n",
    "    for result in search(query, num_results=5):\n",
    "        if result.startswith(\"http\"):\n",
    "            return result\n",
    "    return None\n",
    "\n",
    "def extract_emails_from_url(url, visited):\n",
    "    emails = set()\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Extract emails from page text\n",
    "        text = soup.get_text()\n",
    "        emails.update(re.findall(EMAIL_REGEX, text))\n",
    "\n",
    "        # Find internal links to crawl (contact/about)\n",
    "        base_domain = urlparse(url).netloc\n",
    "        for link in soup.find_all(\"a\", href=True):\n",
    "            href = link[\"href\"].lower()\n",
    "            if any(x in href for x in [\"contact\", \"about\"]):\n",
    "                full_url = urljoin(url, link[\"href\"])\n",
    "                if full_url not in visited and base_domain in full_url:\n",
    "                    visited.add(full_url)\n",
    "                    emails.update(extract_emails_from_url(full_url, visited))\n",
    "\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return emails\n",
    "\n",
    "def get_company_emails(company_name):\n",
    "    website = find_website(company_name)\n",
    "    if not website:\n",
    "        return None, set()\n",
    "\n",
    "    visited = {website}\n",
    "    emails = extract_emails_from_url(website, visited)\n",
    "    return website, emails\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    company = input(\"Enter company name: \").strip()\n",
    "    website, emails = get_company_emails(company)\n",
    "\n",
    "    if website:\n",
    "        print(f\"\\nWebsite: {website}\")\n",
    "    else:\n",
    "        print(\"\\nWebsite not found\")\n",
    "\n",
    "    if emails:\n",
    "        print(\"\\nEmails found:\")\n",
    "        for email in emails:\n",
    "            print(email)\n",
    "    else:\n",
    "        print(\"\\nNo emails found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting googlesearch-python\n",
      "  Using cached googlesearch_python-1.3.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: beautifulsoup4>=4.9 in c:\\users\\hak3\\anaconda3\\lib\\site-packages (from googlesearch-python) (4.12.3)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\hak3\\anaconda3\\lib\\site-packages (from googlesearch-python) (2.32.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hak3\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.9->googlesearch-python) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hak3\\anaconda3\\lib\\site-packages (from requests>=2.20->googlesearch-python) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hak3\\anaconda3\\lib\\site-packages (from requests>=2.20->googlesearch-python) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hak3\\anaconda3\\lib\\site-packages (from requests>=2.20->googlesearch-python) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hak3\\anaconda3\\lib\\site-packages (from requests>=2.20->googlesearch-python) (2025.4.26)\n",
      "Using cached googlesearch_python-1.3.0-py3-none-any.whl (5.6 kB)\n",
      "Installing collected packages: googlesearch-python\n",
      "Successfully installed googlesearch-python-1.3.0\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install googlesearch-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Website: https://tesla.in\n",
      "\n",
      "No emails found\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\"\n",
    "}\n",
    "\n",
    "EMAIL_REGEX = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
    "\n",
    "COMMON_TLDS = [\"com\", \"in\", \"co\", \"io\", \"net\", \"org\"]\n",
    "\n",
    "def generate_domains(company_name):\n",
    "    base = company_name.lower().replace(\"&\", \"and\").replace(\" \", \"\")\n",
    "    return [f\"https://{base}.{tld}\" for tld in COMMON_TLDS]\n",
    "\n",
    "def is_valid_site(url):\n",
    "    try:\n",
    "        r = requests.get(url, headers=HEADERS, timeout=6)\n",
    "        return r.status_code == 200\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def extract_emails(url, visited):\n",
    "    emails = set()\n",
    "    try:\n",
    "        r = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "        emails.update(re.findall(EMAIL_REGEX, soup.get_text()))\n",
    "\n",
    "        domain = urlparse(url).netloc\n",
    "        for link in soup.find_all(\"a\", href=True):\n",
    "            href = link[\"href\"].lower()\n",
    "            if any(k in href for k in [\"contact\", \"about\"]):\n",
    "                full_url = urljoin(url, link[\"href\"])\n",
    "                if domain in full_url and full_url not in visited:\n",
    "                    visited.add(full_url)\n",
    "                    emails.update(extract_emails(full_url, visited))\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return emails\n",
    "\n",
    "def get_company_emails(company_name):\n",
    "    for site in generate_domains(company_name):\n",
    "        if is_valid_site(site):\n",
    "            visited = {site}\n",
    "            emails = extract_emails(site, visited)\n",
    "            return site, emails\n",
    "\n",
    "    return None, set()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    company = input(\"Enter company name: \").strip()\n",
    "    site, emails = get_company_emails(company)\n",
    "\n",
    "    if site:\n",
    "        print(f\"\\nWebsite: {site}\")\n",
    "    else:\n",
    "        print(\"\\nWebsite not found\")\n",
    "\n",
    "    if emails:\n",
    "        print(\"\\nEmails found:\")\n",
    "        for e in emails:\n",
    "            print(e)\n",
    "    else:\n",
    "        print(\"\\nNo emails found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Website not found\n",
      "\n",
      "No emails found\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\"\n",
    "}\n",
    "\n",
    "EMAIL_REGEX = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
    "\n",
    "MAX_PAGES = 20   # crawl limit per site\n",
    "MAX_DEPTH = 2    # link depth limit\n",
    "\n",
    "def extract_emails(text):\n",
    "    return set(re.findall(EMAIL_REGEX, text))\n",
    "\n",
    "def crawl_site(start_url):\n",
    "    domain = urlparse(start_url).netloc\n",
    "    visited = set()\n",
    "    queue = [(start_url, 0)]\n",
    "    emails = set()\n",
    "\n",
    "    while queue and len(visited) < MAX_PAGES:\n",
    "        url, depth = queue.pop(0)\n",
    "        if url in visited or depth > MAX_DEPTH:\n",
    "            continue\n",
    "\n",
    "        visited.add(url)\n",
    "\n",
    "        try:\n",
    "            r = requests.get(url, headers=HEADERS, timeout=10)\n",
    "            soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "            # extract emails\n",
    "            emails.update(extract_emails(soup.get_text()))\n",
    "\n",
    "            # find all internal links (tabs, menus, footer)\n",
    "            for link in soup.find_all(\"a\", href=True):\n",
    "                href = link[\"href\"]\n",
    "                full_url = urljoin(url, href)\n",
    "                parsed = urlparse(full_url)\n",
    "\n",
    "                if parsed.netloc == domain and full_url not in visited:\n",
    "                    queue.append((full_url, depth + 1))\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return emails\n",
    "\n",
    "def get_company_emails(company_name):\n",
    "    base = company_name.lower().replace(\" \", \"\").replace(\"&\", \"and\")\n",
    "    site = f\"https://{base}.com\"\n",
    "\n",
    "    try:\n",
    "        requests.get(site, headers=HEADERS, timeout=5)\n",
    "    except:\n",
    "        return None, set()\n",
    "\n",
    "    emails = crawl_site(site)\n",
    "    return site, emails\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    company = input(\"Enter company name: \").strip()\n",
    "    site, emails = get_company_emails(company)\n",
    "\n",
    "    if site:\n",
    "        print(f\"\\nWebsite: {site}\")\n",
    "    else:\n",
    "        print(\"\\nWebsite not found\")\n",
    "\n",
    "    if emails:\n",
    "        print(\"\\nEmails found:\")\n",
    "        for e in emails:\n",
    "            print(e)\n",
    "    else:\n",
    "        print(\"\\nNo emails found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'playwright'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m urljoin, urlparse\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplaywright\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msync_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sync_playwright\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[0;32m      6\u001b[0m EMAIL_REGEX \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[a-zA-Z0-9._\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m+-]+@[a-zA-Z0-9.-]+\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.[a-zA-Z]\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m2,}\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'playwright'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from playwright.sync_api import sync_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "EMAIL_REGEX = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
    "OBFUSCATED_REGEX = [\n",
    "    r\"([a-zA-Z0-9._%+-]+)\\s*\\[at\\]\\s*([a-zA-Z0-9.-]+)\\s*\\[dot\\]\\s*([a-zA-Z]{2,})\",\n",
    "    r\"([a-zA-Z0-9._%+-]+)\\s*\\(at\\)\\s*([a-zA-Z0-9.-]+)\\s*\\(dot\\)\\s*([a-zA-Z]{2,})\",\n",
    "]\n",
    "\n",
    "MAX_PAGES = 25\n",
    "MAX_DEPTH = 2\n",
    "\n",
    "def normalize_email(match):\n",
    "    return f\"{match[0]}@{match[1]}.{match[2]}\"\n",
    "\n",
    "def extract_emails(text):\n",
    "    emails = set(re.findall(EMAIL_REGEX, text))\n",
    "    for pattern in OBFUSCATED_REGEX:\n",
    "        for match in re.findall(pattern, text, flags=re.I):\n",
    "            emails.add(normalize_email(match))\n",
    "    return emails\n",
    "\n",
    "def crawl_site(start_url):\n",
    "    visited = set()\n",
    "    queue = [(start_url, 0)]\n",
    "    emails = set()\n",
    "    domain = urlparse(start_url).netloc\n",
    "\n",
    "    with sync_playwright() as p:\n",
    "        browser = p.chromium.launch(headless=True)\n",
    "        page = browser.new_page()\n",
    "\n",
    "        while queue and len(visited) < MAX_PAGES:\n",
    "            url, depth = queue.pop(0)\n",
    "            if url in visited or depth > MAX_DEPTH:\n",
    "                continue\n",
    "\n",
    "            visited.add(url)\n",
    "\n",
    "            try:\n",
    "                page.goto(url, timeout=60000)\n",
    "                page.wait_for_timeout(3000)\n",
    "\n",
    "                html = page.content()\n",
    "                soup = BeautifulSoup(html, \"html.parser\")\n",
    "                text = soup.get_text(separator=\" \")\n",
    "\n",
    "                emails.update(extract_emails(text))\n",
    "\n",
    "                for link in soup.find_all(\"a\", href=True):\n",
    "                    full_url = urljoin(url, link[\"href\"])\n",
    "                    parsed = urlparse(full_url)\n",
    "\n",
    "                    if parsed.netloc == domain and full_url not in visited:\n",
    "                        queue.append((full_url, depth + 1))\n",
    "\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        browser.close()\n",
    "\n",
    "    return emails\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    website = input(\"Enter company website (https://...): \").strip()\n",
    "    emails = crawl_site(website)\n",
    "\n",
    "    if emails:\n",
    "        print(\"\\nEmails found:\")\n",
    "        for e in sorted(emails):\n",
    "            print(e)\n",
    "    else:\n",
    "        print(\"\\nNo public emails found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from playwright.sync_api import sync_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Regex patterns\n",
    "EMAIL_REGEX = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
    "OBFUSCATED_PATTERNS = [\n",
    "    r\"([a-zA-Z0-9._%+-]+)\\s*\\[at\\]\\s*([a-zA-Z0-9.-]+)\\s*\\[dot\\]\\s*([a-zA-Z]{2,})\",\n",
    "    r\"([a-zA-Z0-9._%+-]+)\\s*\\(at\\)\\s*([a-zA-Z0-9.-]+)\\s*\\(dot\\)\\s*([a-zA-Z]{2,})\",\n",
    "]\n",
    "\n",
    "MAX_PAGES = 30\n",
    "MAX_DEPTH = 3\n",
    "\n",
    "def extract_emails(text):\n",
    "    emails = set(re.findall(EMAIL_REGEX, text))\n",
    "    for pattern in OBFUSCATED_PATTERNS:\n",
    "        for m in re.findall(pattern, text, flags=re.I):\n",
    "            emails.add(f\"{m[0]}@{m[1]}.{m[2]}\")\n",
    "    return emails\n",
    "\n",
    "def crawl_website(start_url):\n",
    "    visited = set()\n",
    "    queue = [(start_url, 0)]\n",
    "    found_emails = set()\n",
    "    domain = urlparse(start_url).netloc\n",
    "\n",
    "    with sync_playwright() as p:\n",
    "        browser = p.chromium.launch(headless=True)\n",
    "        page = browser.new_page()\n",
    "\n",
    "        while queue and len(visited) < MAX_PAGES:\n",
    "            url, depth = queue.pop(0)\n",
    "            if url in visited or depth > MAX_DEPTH:\n",
    "                continue\n",
    "\n",
    "            visited.add(url)\n",
    "\n",
    "            try:\n",
    "                page.goto(url, timeout=60000)\n",
    "                page.wait_for_timeout(2500)\n",
    "\n",
    "                html = page.content()\n",
    "                soup = BeautifulSoup(html, \"html.parser\")\n",
    "                text = soup.get_text(separator=\" \")\n",
    "\n",
    "                found_emails.update(extract_emails(text))\n",
    "\n",
    "                for link in soup.find_all(\"a\", href=True):\n",
    "                    full_url = urljoin(url, link[\"href\"])\n",
    "                    parsed = urlparse(full_url)\n",
    "\n",
    "                    if parsed.netloc == domain and full_url not in visited:\n",
    "                        queue.append((full_url, depth + 1))\n",
    "\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        browser.close()\n",
    "\n",
    "    return found_emails\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    website = input(\"Enter website URL (https://...): \").strip()\n",
    "    emails = crawl_website(website)\n",
    "\n",
    "    if emails:\n",
    "        print(\"\\nEmails found:\")\n",
    "        for email in sorted(emails):\n",
    "            print(email)\n",
    "    else:\n",
    "        print(\"\\nNo public emails found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No public emails found\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from collections import deque\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\"\n",
    "}\n",
    "\n",
    "EMAIL_REGEX = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
    "\n",
    "OBFUSCATED_REGEX = [\n",
    "    r\"([a-zA-Z0-9._%+-]+)\\s*\\[at\\]\\s*([a-zA-Z0-9.-]+)\\s*\\[dot\\]\\s*([a-zA-Z]{2,})\",\n",
    "    r\"([a-zA-Z0-9._%+-]+)\\s*\\(at\\)\\s*([a-zA-Z0-9.-]+)\\s*\\(dot\\)\\s*([a-zA-Z]{2,})\",\n",
    "]\n",
    "\n",
    "MAX_PAGES = 50\n",
    "\n",
    "def extract_emails(text):\n",
    "    emails = set(re.findall(EMAIL_REGEX, text))\n",
    "    for pattern in OBFUSCATED_REGEX:\n",
    "        for m in re.findall(pattern, text, flags=re.I):\n",
    "            emails.add(f\"{m[0]}@{m[1]}.{m[2]}\")\n",
    "    return emails\n",
    "\n",
    "def crawl_website(start_url):\n",
    "    visited = set()\n",
    "    queue = deque([start_url])\n",
    "    domain = urlparse(start_url).netloc\n",
    "    found_emails = set()\n",
    "\n",
    "    while queue and len(visited) < MAX_PAGES:\n",
    "        url = queue.popleft()\n",
    "        if url in visited:\n",
    "            continue\n",
    "\n",
    "        visited.add(url)\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            text = soup.get_text(separator=\" \")\n",
    "            found_emails.update(extract_emails(text))\n",
    "\n",
    "            for link in soup.find_all(\"a\", href=True):\n",
    "                full_url = urljoin(url, link[\"href\"])\n",
    "                parsed = urlparse(full_url)\n",
    "\n",
    "                if parsed.scheme in [\"http\", \"https\"] and parsed.netloc == domain:\n",
    "                    if full_url not in visited:\n",
    "                        queue.append(full_url)\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return found_emails\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    website = input(\"Enter website URL (https://...): \").strip()\n",
    "    emails = crawl_website(website)\n",
    "\n",
    "    if emails:\n",
    "        print(\"\\nEmails found:\")\n",
    "        for e in sorted(emails):\n",
    "            print(e)\n",
    "    else:\n",
    "        print(\"\\nNo public emails found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting multithreaded crawl...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HAK3\\AppData\\Local\\Temp\\ipykernel_16156\\1011411347.py:95: DeprecationWarning: The 'registered_domain' property is deprecated and will be removed in the next major version. Use 'top_domain_under_public_suffix' instead, which has the same behavior but a more accurate name.\n",
      "  base_domain = tldextract.extract(start_url).registered_domain\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling: https://tesla.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HAK3\\AppData\\Local\\Temp\\ipykernel_16156\\1011411347.py:79: DeprecationWarning: The 'registered_domain' property is deprecated and will be removed in the next major version. Use 'top_domain_under_public_suffix' instead, which has the same behavior but a more accurate name.\n",
      "  domain = tldextract.extract(link).registered_domain\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling: https://www.tesla.com//support/browser-support\n",
      "Crawling: https://shop.tesla.com\n",
      "Crawling: https://tesla.com/support\n",
      "Crawling: https://tesla.com/teslaaccount\n",
      "Crawling: https://tesla.com/models\n",
      "Crawling: https://tesla.com/models/design\n",
      "Crawling: https://tesla.com/model3\n",
      "Crawling: https://tesla.com/model3/design\n",
      "Crawling: https://tesla.com/modely\n",
      "\n",
      "Crawl completed.\n",
      "Pages crawled: 10\n",
      "\n",
      "Emails found:\n",
      "Homepage-Fe@ures-Desktop.png\n",
      "Homepage-Fe@ures-Mobile.png\n",
      "Homepage-Fe@ures-Tablet.png\n",
      "apigateway-pricing-g@eway.tesla.com\n",
      "ch@-loader.js\n",
      "cua-ch@-ui.tesla.com\n",
      "loc@ion-script.js\n",
      "window.loc@ion.href\n",
      "window.navig@or.cookieEnabled\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "from queue import Queue\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from threading import Thread, Lock\n",
    "\n",
    "import tldextract\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Regex patterns\n",
    "EMAIL_REGEX = re.compile(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\")\n",
    "OBFUSCATED_REGEX = re.compile(\n",
    "    r\"([a-zA-Z0-9._%+-]+)\\s*(?:\\[at\\]|\\(at\\)|at)\\s*\"\n",
    "    r\"([a-zA-Z0-9.-]+)\\s*(?:\\[dot\\]|\\(dot\\)|dot|\\.)\\s*\"\n",
    "    r\"([a-zA-Z]{2,})\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "NUM_THREADS = 3  # number of browser threads\n",
    "\n",
    "def normalize_url(url):\n",
    "    if not url.startswith(\"http\"):\n",
    "        url = \"https://\" + url\n",
    "    return url.rstrip(\"/\")\n",
    "\n",
    "def setup_driver():\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    options.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
    "    )\n",
    "    return webdriver.Chrome(\n",
    "        service=Service(ChromeDriverManager().install()),\n",
    "        options=options\n",
    "    )\n",
    "\n",
    "def worker(queue, visited, emails, lock, base_domain, max_pages):\n",
    "    driver = setup_driver()\n",
    "    while True:\n",
    "        try:\n",
    "            url = queue.get(timeout=5)\n",
    "        except:\n",
    "            break  # queue empty\n",
    "\n",
    "        with lock:\n",
    "            if url in visited or len(visited) >= max_pages:\n",
    "                queue.task_done()\n",
    "                continue\n",
    "            visited.add(url)\n",
    "\n",
    "        print(f\"Crawling: {url}\")\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            time.sleep(2)  # allow JS\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "            # Extract emails\n",
    "            found_emails = set(EMAIL_REGEX.findall(html))\n",
    "            for match in OBFUSCATED_REGEX.findall(html):\n",
    "                found_emails.add(f\"{match[0]}@{match[1]}.{match[2]}\")\n",
    "\n",
    "            with lock:\n",
    "                emails.update(found_emails)\n",
    "\n",
    "            # Find internal links\n",
    "            for a in soup.find_all(\"a\", href=True):\n",
    "                link = urljoin(url, a[\"href\"])\n",
    "                parsed = urlparse(link)\n",
    "                if parsed.scheme not in (\"http\", \"https\"):\n",
    "                    continue\n",
    "                domain = tldextract.extract(link).registered_domain\n",
    "                if domain == base_domain:\n",
    "                    clean_link = link.split(\"#\")[0].rstrip(\"/\")\n",
    "                    with lock:\n",
    "                        if clean_link not in visited:\n",
    "                            queue.put(clean_link)\n",
    "\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        finally:\n",
    "            queue.task_done()\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "def crawl_website_multithread(start_url, max_pages=10):\n",
    "    start_url = normalize_url(start_url)\n",
    "    base_domain = tldextract.extract(start_url).registered_domain\n",
    "    visited = set()\n",
    "    emails = set()\n",
    "    lock = Lock()\n",
    "    queue = Queue()\n",
    "    queue.put(start_url)\n",
    "\n",
    "    threads = []\n",
    "    for _ in range(NUM_THREADS):\n",
    "        t = Thread(target=worker, args=(queue, visited, emails, lock, base_domain, max_pages))\n",
    "        t.daemon = True\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "\n",
    "    queue.join() \n",
    "\n",
    "    return sorted(emails), len(visited)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    website_input = input(\"Enter website URL (example.com): \").strip()\n",
    "    print(\"\\nStarting multithreaded crawl...\\n\")\n",
    "    emails, pages = crawl_website_multithread(website_input)\n",
    "\n",
    "    print(\"\\nCrawl completed.\")\n",
    "    print(f\"Pages crawled: {pages}\")\n",
    "    if emails:\n",
    "        print(\"\\nEmails found:\")\n",
    "        for e in emails:\n",
    "            print(e)\n",
    "    else:\n",
    "        print(\"\\nNo emails found on the website.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emails Found:\n",
      "care@stylo.pkCustomer\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# User se URL input lena\n",
    "url = input(\"Enter the URL to scrape emails from: \")\n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Check if request was successful\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error fetching URL: {e}\")\n",
    "    exit()\n",
    "\n",
    "html_content = response.text\n",
    "\n",
    "# Parse HTML\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# Get text only\n",
    "text = soup.get_text()\n",
    "\n",
    "# Regex for email extraction\n",
    "emails = re.findall(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\", text)\n",
    "\n",
    "# Remove duplicates\n",
    "unique_emails = set(emails)\n",
    "\n",
    "if unique_emails:\n",
    "    print(\"Emails Found:\")\n",
    "    for email in unique_emails:\n",
    "        print(email)\n",
    "else:\n",
    "    print(\"No emails found on this page.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling full site: https://www.gnu.org\n",
      "Crawling full site: https://www.python.org\n",
      "Crawling full site: https://www.apache.org\n",
      "Crawling full site: https://www.linuxfoundation.org\n",
      "Crawling full site: https://www.wikipedia.org\n",
      "Crawling full site: https://www.mozilla.org\n",
      "Crawling full site: https://www.djangoproject.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HAK3\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\html\\parser.py:171: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling full site: https://www.postgresql.org\n",
      "Crawling full site: https://www.mysql.com\n",
      "Crawling full site: https://www.php.net\n",
      "Full website crawling completed.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\"\n",
    "}\n",
    "\n",
    "def is_internal_link(base_url, link):\n",
    "    return urlparse(base_url).netloc == urlparse(link).netloc\n",
    "\n",
    "def extract_emails_from_site(base_url, max_pages=20):\n",
    "    visited = set()\n",
    "    to_visit = [base_url]\n",
    "    found_emails = set()\n",
    "\n",
    "    while to_visit and len(visited) < max_pages:\n",
    "        url = to_visit.pop(0)\n",
    "        if url in visited:\n",
    "            continue\n",
    "\n",
    "        visited.add(url)\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        text = soup.get_text()\n",
    "        emails = re.findall(\n",
    "            r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\",\n",
    "            text\n",
    "        )\n",
    "        found_emails.update(emails)\n",
    "\n",
    "        # Collect internal links (tabs/pages)\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            link = urljoin(base_url, a[\"href\"])\n",
    "            if is_internal_link(base_url, link) and link not in visited:\n",
    "                to_visit.append(link)\n",
    "\n",
    "    return found_emails\n",
    "\n",
    "# Excel Input\n",
    "df = pd.read_excel(\"companies.xlsx\")\n",
    "df[\"Emails_Found\"] = \"\"\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    url = row[\"Company_URL\"]\n",
    "    print(f\"Crawling full site: {url}\")\n",
    "\n",
    "    emails = extract_emails_from_site(url)\n",
    "\n",
    "    if emails:\n",
    "        df.at[index, \"Emails_Found\"] = \", \".join(emails)\n",
    "    else:\n",
    "        df.at[index, \"Emails_Found\"] = \"No email found\"\n",
    "\n",
    "df.to_excel(\"companies_with_emails.xlsx\", index=False)\n",
    "\n",
    "print(\"Full website crawling completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
